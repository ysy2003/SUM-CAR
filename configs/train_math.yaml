base_model: meta-llama/Llama-3.2-1B
mem:
  num_slots: 2048  # Minimal for v3-8
  k_top: 8  # Minimal
  alpha: 1.0
  tau: 10.0
  use_gate: true
  normalize_retrieval: true
train:
  epochs: 3
  lr: 5.0e-4
  lr_kv: 1.0e-3
  lr_gate: 1.0e-4
  batch_size: 1  # Absolute minimum
  max_length: 128  # Much smaller
  probe_steps: 200  # Faster
  top_t: 256  # Minimal
  refresh_every: 50
  save_dir: out/math
  seed: 42
  dataset: gsm8k
  probe_steps: 1000
  top_t: 2048
  save_dir: out/math
  seed: 42
  dataset: gsm8k
