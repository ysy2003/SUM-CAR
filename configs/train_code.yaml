base_model: gpt2  # 124M params - fits in single TPU core
mem:
  num_slots: 2048  # Reduced for TPU v6e-1 memory
  k_top: 4  # Reduced to save memory
  alpha: 1.0
  tau: 10.0
  use_gate: false  # Disable to save memory
  normalize_retrieval: true
  track_hits: false  # Must disable on TPU
  hits_source: "none"
train:
  epochs: 3
  lr: 5.0e-4
  lr_kv: 1.0e-3
  lr_gate: 1.0e-4
  batch_size: 8
  max_length: 128  # 代码序列较短，可以用更大batch
  probe_steps: 200  # Reduced from 300
  top_t: 256  # Reduced from 512
  refresh_every: 100
  save_dir: out/code
  seed: 42
  dataset: codexglue_refine
  max_examples: 500  # Limit for faster iteration