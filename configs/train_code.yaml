base_model: meta-llama/Llama-3.2-1B
mem:
num_slots: 65536
k_top: 32
alpha: 1.0
train:
epochs: 1
lr: 5.0e-4
batch_size: 2
max_length: 768
probe_steps: 1000
top_t: 4096
save_dir: out/code
seed: 42
dataset: codexglue_refine # or mbpp =