base_model: meta-llama/Llama-3.2-1B
mem:
num_slots: 65536
k_top: 32
alpha: 1.0
train:
epochs: 1
lr: 5.0e-4
batch_size: 2
max_length: 768
probe_steps: 800
top_t: 2048
save_dir: out/finqa
seed: 42
dataset: finqa_rc